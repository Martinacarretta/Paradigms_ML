{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6899a31-5c75-4956-b87f-d39bb7ae42de",
   "metadata": {},
   "source": [
    "# Practical Activity 1 (**PRA1**)\n",
    "\n",
    "## Evaluable Practical Exercise\n",
    "\n",
    "<u>General considerations</u>:\n",
    "\n",
    "- The proposed solution cannot use methods, functions or parameters declared **_deprecated_** in future versions.\n",
    "- This activity must be carried out on a **strictly individual** basis. Any indication of copying will be penalized with a failure for all parties involved and the possible negative evaluation of the subject in its entirety.\n",
    "- It is necessary for the student to indicate **all the sources** that she/he has used to carry out the PRA. If not, the student will be considered to have committed plagiarism, being penalized with a failure and the possible negative evaluation of the subject in its entirety.\n",
    "\n",
    "<u>Delivery format</u>:\n",
    "\n",
    "- Some exercises may require several minutes of execution, so the delivery must be done in **Notebook format** and in **HTML format**, where the code, results and comments of each exercise can be seen. You can export the notebook to HTML from the menu File $\\to$ Download as $\\to$ HTML.\n",
    "- There is a special type of cell to hold text. This type of cell will be very useful to answer the different theoretical questions posed throughout the activity. To change the cell type to this type, in the menu: Cell $\\to$ Cell Type $\\to$ Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1d0b9-51a4-4e3c-9bd8-0a8fd47d5a0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Name and surname: Martina Carretta</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f1545-5222-41a9-9a84-f7ed1d93cf4b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Blackjack environment is part of the Gymnasium's [Toy Text](https://gymnasium.farama.org/environments/toy_text/) environments. Blackjack is a card game where the goal is to beat the dealer by obtaining cards that sum to closer to 21 (without going over 21) than the dealer's cards.\n",
    "\n",
    "The game starts with the dealer having one face up and one face down card, while the player has two face up cards. All cards are drawn from an infinite deck (i.e. with replacement).\n",
    "\n",
    "The card values are, as depicted in the following figure:\n",
    "- Face cards (Jack, Queen, King) have a point value of **10**.\n",
    "- Aces can either count as **11** (called a ``usable ace'') or **1**.\n",
    "- Numerical cards (**2-9**) have a value equal to their number.\n",
    "\n",
    "<img src=\"./figs/BlackJackCards.png\" />\n",
    "\n",
    "The player has the sum of cards held. The player can request additional cards (**hit**) until they decide to stop (**stick**) or exceed 21 (**bust**, immediate loss).\n",
    "\n",
    "After the player sticks, the dealer reveals their face down card, and draws cards until their sum is 17 or greater. If the dealer goes bust, the player wins.\n",
    "\n",
    "If neither the player nor the dealer busts, the outcome (win, lose, draw) is decided by whose sum is closer to 21.\n",
    "\n",
    "Further information could be found at:\n",
    "- Gymnasium [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/)\n",
    "\n",
    "In order to initialize the environment, we will use `natural=True` to give an additional reward for starting with a natural blackjack, i.e. starting with an ace and ten (sum is 21), as depicted in the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf8ef26-79d7-4600-abb4-bd82cf12f8fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('Blackjack-v1', natural=True, sab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d932fb-fbf0-4d42-a50d-f6887211e1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(2) \n",
      "Observation space is Tuple(Discrete(32), Discrete(11), Discrete(2)) \n",
      "Reward range is (-inf, inf) \n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))\n",
    "print(\"Reward range is {} \".format(env.reward_range))\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77be78-4195-4282-81d2-3ccb63dd9e87",
   "metadata": {},
   "source": [
    "## Part 1. Naïve Policy\n",
    "\n",
    "Implement an agent that carries out the following deterministic policy: \n",
    "- The agent will **stick** if it gets a score of 20 or 21.\n",
    "- Otherwise, it will **hit**.\n",
    "\n",
    "<u>Questions</u> (**1 point**): \n",
    "1. Using this agent, simulate 100,000 games and calculate the agent's return (total accumulated reward).\n",
    "2. Additionally, calculate the % of wins, natural wins, losses and draws. \n",
    "3. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fd6ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state = self.env.reset()[0]\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "\n",
    "    def select_action(self, state) -> int:\n",
    "        score = self.calculate_score(state)\n",
    "        if score == 20 or score == 21:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "\n",
    "    def calculate_score(self, state) -> int:\n",
    "        player_sum, dealer_card, usable_ace = state\n",
    "        return player_sum\n",
    "\n",
    "\n",
    "    def play_episode(self, env) -> float:\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            is_done = terminated or truncated\n",
    "            if is_done:\n",
    "                total_reward = int(reward) # only compute once since the reward is only given at the end -1, 0, or 1\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "    \n",
    "agent = Agent(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce4657c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accumulated reward: -35823.0\n"
     ]
    }
   ],
   "source": [
    "num_games = 100000\n",
    "\n",
    "total_reward = 0.0\n",
    "\n",
    "for _ in range(num_games):\n",
    "    total_reward += agent.play_episode(env)\n",
    "\n",
    "print(f\"Total accumulated reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefad798",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m draws \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_games):\n\u001b[0;32m----> 9\u001b[0m     state, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     10\u001b[0m     episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "num_games = 100000\n",
    "wins = 0\n",
    "natural_wins = 0\n",
    "losses = 0\n",
    "draws = 0\n",
    "\n",
    "for _ in range(num_games):\n",
    "    state, _ = agent.env.reset()\n",
    "    episode_reward = 0.0\n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "        new_state, reward, terminated, truncated, _ = agent.env.step(action)\n",
    "        is_done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        if is_done:\n",
    "            if reward == 1.0:\n",
    "                wins += 1\n",
    "                if state[0] == 21 and state[2]:  # Natural win\n",
    "                    natural_wins += 1\n",
    "            elif reward == -1.0:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "win_percentage = (wins / num_games) * 100\n",
    "natural_win_percentage = (natural_wins / num_games) * 100\n",
    "loss_percentage = (losses / num_games) * 100\n",
    "draw_percentage = (draws / num_games) * 100\n",
    "\n",
    "print(f\"Win percentage: {win_percentage}%\")\n",
    "print(f\"Natural win percentage: {natural_win_percentage}%\")\n",
    "print(f\"Loss percentage: {loss_percentage}%\")\n",
    "print(f\"Draw percentage: {draw_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b30e3-dd3d-4fcd-a6e4-baaf764d8f88",
   "metadata": {},
   "source": [
    "## Part 2. Monte Carlo method\n",
    "\n",
    "The objective of this section is to estimate the optimal policy using Monte Carlo methods. Specifically, you can choose and implement one of the algorithms related to _Control using MC methods_ (with ''exploring starts'' or without ''exploring starts'', both on-policy or off-policy).\n",
    "\n",
    "<u>Questions</u> (**2.5 points**): \n",
    "1. Implement the selected algorithm and justify your choice.\n",
    "2. Comment and justify all the parameters, such as:\n",
    "- Number of episodes\n",
    "- Discount factor\n",
    "- Etc.\n",
    "3. Implement a function that prints on the screen the optimal policy found for each state (similar to the figure in Section 3.1).\n",
    "4. Using the trained agent, simulate 100,000 games and calculate the agent's return (total accumulated reward).\n",
    "5. Additionally, calculate the % of wins, natural wins, losses and draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6904e-2cd2-4f95-b2ce-4fb8a9dd7b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1bb6acf-25c2-4f01-b2da-ca7b16435266",
   "metadata": {},
   "source": [
    "## Part 3. TD learning\n",
    "\n",
    "The objective of this section is to estimate the optimal policy using TD learning methods. Specifically, you have to implement the SARSA algorithm.\n",
    "\n",
    "<u>Questions</u> (**2.5 points**): \n",
    "1. Implement the algorithm.\n",
    "2. Comment and justify all the parameters.\n",
    "3. Print on the screen the optimal policy found for each state.\n",
    "4. Using the trained agent, simulate 100,000 games and calculate the agent's return (total accumulated reward).\n",
    "5. Additionally, calculate the % of wins, natural wins, losses and draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa4fee-c8e8-4ffa-b8e9-5c75cde0aef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c77db3-3fd1-4454-8079-e2eb44942758",
   "metadata": {},
   "source": [
    "## Part 4. Comparison of the algorithms\n",
    "\n",
    "In this section, we will make a comparison among the algorithms.\n",
    "\n",
    "We will compare the performance of the algorithms when changing the number of episodes, the discount factor and the *learning rate* values (in the case of the SARSA method).\n",
    "\n",
    "For each exercise, the results must be presented and justified.\n",
    "\n",
    "**Note**: \n",
    "- It is recommended to run the simulations multiple times for each exercise, as these are random, and to comment on the most frequent result or the average of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1e4c9-29a8-4552-a463-e4e6cf2b7e91",
   "metadata": {},
   "source": [
    "### 4.1. Comparison to the optimal policy\n",
    "\n",
    "The optimal policy for this problem, described by [Sutton & Barto](http://incompleteideas.net/book/the-book-2nd.html) is depicted in the following image:\n",
    "\n",
    "<img src=\"./figs/optimal.png\" style=\"width: 800px;\" />\n",
    "\n",
    "<u>Questions</u> (**1 point**): \n",
    "- Compare the _optimal_ policies of the naïve, Monte Carlo and SARSA methods to the optimal one provided by Sutton & Barto.\n",
    "- Comment on the results and justify your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148630a-af42-4b8a-9754-088340fa65a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177e4a86-98bc-4feb-84b0-5cf85a27ca17",
   "metadata": {},
   "source": [
    "### 4.2. Influence of the Number of Episodes\n",
    "\n",
    "Conduct a study by varying the number of episodes in each of the algorithms.\n",
    "\n",
    "<u>Questions</u> (**1 point**): \n",
    "- Train each algorithm multiple times with 100,000, 1,000,000, and 5,000,000 episodes and average the results.\n",
    "- Indicate how the **number of episodes** influences the convergence of each algorithm by calculating the number of states where the policy differs from the optimal one, as well as the average return obtained after playing 100,000 games following each training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a296163-fdf1-4f83-837c-9c7e95946805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cbfc6b9-a71e-4665-b124-88bfd6390aef",
   "metadata": {},
   "source": [
    "### 4.3. Influence of the Discount Factor\n",
    "\n",
    "Conduct a study by varying the *discount factor* in each of the algorithms.\n",
    "\n",
    "<u>Questions</u> (**1 point**):\n",
    "- Run the algorithms with *discount factor* = 0.1, 0.5, 0.9 and the rest of the parameters the same as in previous exercises. \n",
    "- Describe the changes in the optimal policy, comparing the result obtained with the result of previous exercises (*discount factor* = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e716e-5234-4e16-8a74-febda9a0b0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9afe8b5-8a12-4c1d-a9ab-75da38fe5aee",
   "metadata": {},
   "source": [
    "### 4.4. Influence of the Learning Rate\n",
    "\n",
    "Conduct a study by varying the learning rate in the *SARSA* algorithm.\n",
    "\n",
    "<u>Questions</u> (**1 point**):\n",
    "- Run the *SARSA* algorithm with the following *learning rate* values: 0.001, 0.01, 0.1, and 0.9.\n",
    "- Analyze the differences with the results obtained previously in terms of the number of errors relative to the optimal policy and the accumulated reward for every 100,000 episodes played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408beff-0204-42a2-b50f-6c380f24c8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
