{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tabular Reinforcement Learning**\n",
    "\n",
    "# Q-Learning on FrozenLake environment\n",
    "\n",
    "## Non-Evaluables Practical Exercices\n",
    "\n",
    "This is a non-evaluable practical exercise, but it is recommended that students complete it fully and individually, since it is an important part of the learning process.\n",
    "\n",
    "The solution will be available, although it is not recommended that students consult the solution until they have completed the exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FrozenLake environment\n",
    "\n",
    "In this activity, we are going to solve the [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment.\n",
    "\n",
    "Main characteristics:\n",
    "- The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "- Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
    "- The player makes moves until they reach the goal or fall in a hole.\n",
    "- The lake is slippery (unless disabled) so the player may move perpendicular to the intended direction sometimes (see _is_slippery_ param).\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning algorithm\n",
    "\n",
    "<u>Question 1</u>: : **Implement the *Q-Learning* algorithm** using the following parameters:\n",
    "\n",
    "- Number of episodes = 15,000\n",
    "- *learning rate* = 0.8\n",
    "- *discount factor* = 1\n",
    "\n",
    "Additionally, implement de **$\\epsilon$-Greedy with decay factor** method with the following parameters:\n",
    "- *epsilon* = 1.0\n",
    "- *max_epsilon* = 1.0\n",
    "- *min_epsilon* = 0.01\n",
    "- *decay_rate* = 0.005\n",
    "\n",
    "<u>Question 2</u>: Once you have coded the algorithm, try different **values for the hyperparameters** and comment the best ones (providing an empirical comparison):\n",
    "\n",
    "- Number of episodes\n",
    "- *learning rate* \n",
    "- *discount factor* \n",
    "- *epsilon* values (including min value and decay factor)\n",
    "\n",
    "<u>Question 3</u>: Try to solve the same environment but using a _8 x 8_ grid (also in slippery mode):\n",
    "> gym.make(ENV_NAME, desc=None, map_name=\"8x8\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(4) \n",
      "Observation space is Discrete(16) \n",
      "Reward range is (0, 1) \n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# definig the environment\n",
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))\n",
    "print(\"Reward range is {} \".format(env.reward_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
    "    '''\n",
    "    Create a policy where epsilon dictates the probability of a random action being carried out.\n",
    "\n",
    "    :param Q: linka state -> action value (dictionary)\n",
    "    :param state: state in which the agent is (int)\n",
    "    :param nA: number of actions (int)\n",
    "    :param epsilon: possibility of random movement (float)\n",
    "    :return: probability of each action (list) d\n",
    "    '''\n",
    "    probs = np.ones(nA) * epsilon / nA\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += 1.0 - epsilon\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "def QLearning(episodes, learning_rate, discount, epsilon, decay_rate=0.005, max_epsilon = 1, min_epsilon=0.01):\n",
    "    '''\n",
    "    Learn to solve the environment using the Q-Learning algorithm\n",
    "\n",
    "    :param episodes: Number of episodes (int)\n",
    "    :param learning_rate: Learning rate (float [0, 1])\n",
    "    :param discount: Discount factor (float [0, 1])\n",
    "    :param epsilon: chance that random movement is required (float [0, 1])\n",
    "    :return: x,y number of episodes and number of steps\n",
    "    :Q: action value function\n",
    "    '''\n",
    "\n",
    "    # Link actions to states\n",
    "    Q = np.zeros((16, 4))\n",
    "\n",
    "    # Number of episodes\n",
    "    x = np.arange(episodes) \n",
    "    y = np.zeros(episodes)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        epsilon = min (1, epsilon)\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        done = False\n",
    "        step = 1\n",
    "                \n",
    "        while not done:\n",
    "            probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action) # as it's inherited from gym and not gymnasium, we receive only the done, instead of terminated and truncated. \n",
    "\n",
    "            # Update TD\n",
    "            td_target = reward + discount * np.max(Q[next_state, :])\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += learning_rate * td_error\n",
    "\n",
    "            done = terminated or truncated\n",
    "                \n",
    "            if done:\n",
    "                y[episode] = step\n",
    "                break\n",
    "            \n",
    "            total_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "            #action = next_action\n",
    "            step += 1\n",
    "        \n",
    "        epsilon = min_epsilon + (max_epsilon-min_epsilon) * np.exp(-decay_rate*count)\n",
    "\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Episode: {count+1} and Epsilon: {epsilon}, reward: {total_reward}\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 and Epsilon: 1.0, reward: 0.0\n",
      "Episode: 101 and Epsilon: 0.6104653531155071, reward: 0.0\n",
      "Episode: 201 and Epsilon: 0.374200646759728, reward: 0.0\n",
      "Episode: 301 and Epsilon: 0.23089885854694553, reward: 0.0\n",
      "Episode: 401 and Epsilon: 0.14398193040424656, reward: 0.0\n",
      "Episode: 501 and Epsilon: 0.0912641486376598, reward: 0.0\n",
      "Episode: 601 and Epsilon: 0.0592891976841853, reward: 0.0\n",
      "Episode: 701 and Epsilon: 0.039895409588095315, reward: 0.0\n",
      "Episode: 801 and Epsilon: 0.028132482499846838, reward: 0.0\n",
      "Episode: 901 and Epsilon: 0.020997906572859885, reward: 0.0\n",
      "Episode: 1001 and Epsilon: 0.016670567529094613, reward: 0.0\n",
      "Episode: 1101 and Epsilon: 0.014045903724079427, reward: 0.0\n",
      "Episode: 1201 and Epsilon: 0.012453964654899695, reward: 0.0\n",
      "Episode: 1301 and Epsilon: 0.011488404801047796, reward: 0.0\n",
      "Episode: 1401 and Epsilon: 0.010902763145898971, reward: 0.0\n",
      "Episode: 1501 and Epsilon: 0.010547553526446355, reward: 0.0\n",
      "Episode: 1601 and Epsilon: 0.010332108001623487, reward: 0.0\n",
      "Episode: 1701 and Epsilon: 0.010201433685320538, reward: 0.0\n",
      "Episode: 1801 and Epsilon: 0.010122175706045813, reward: 0.0\n",
      "Episode: 1901 and Epsilon: 0.010074103311588823, reward: 0.0\n",
      "Episode: 2001 and Epsilon: 0.010044945930464861, reward: 0.0\n",
      "Episode: 2101 and Epsilon: 0.01002726108485625, reward: 0.0\n",
      "Episode: 2201 and Epsilon: 0.010016534683782344, reward: 0.0\n",
      "Episode: 2301 and Epsilon: 0.010010028792662645, reward: 0.0\n",
      "Episode: 2401 and Epsilon: 0.010006082770229794, reward: 0.0\n",
      "Episode: 2501 and Epsilon: 0.010003689386640358, reward: 0.0\n",
      "Episode: 2601 and Epsilon: 0.010002237726112912, reward: 0.0\n",
      "Episode: 2701 and Epsilon: 0.01000135724949552, reward: 0.0\n",
      "Episode: 2801 and Epsilon: 0.010000823213431913, reward: 0.0\n",
      "Episode: 2901 and Epsilon: 0.010000499304185942, reward: 0.0\n",
      "Episode: 3001 and Epsilon: 0.010000302843297297, reward: 0.0\n",
      "Episode: 3101 and Epsilon: 0.010000183683744899, reward: 0.0\n",
      "Episode: 3201 and Epsilon: 0.010000111409822971, reward: 0.0\n",
      "Episode: 3301 and Epsilon: 0.010000067573473426, reward: 0.0\n",
      "Episode: 3401 and Epsilon: 0.010000040985383415, reward: 0.0\n",
      "Episode: 3501 and Epsilon: 0.010000024858891642, reward: 0.0\n",
      "Episode: 3601 and Epsilon: 0.010000015077679947, reward: 0.0\n",
      "Episode: 3701 and Epsilon: 0.010000009145075166, reward: 0.0\n",
      "Episode: 3801 and Epsilon: 0.010000005546768474, reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43mQLearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 59\u001b[0m, in \u001b[0;36mQLearning\u001b[0;34m(episodes, learning_rate, discount, epsilon, decay_rate, max_epsilon, min_epsilon)\u001b[0m\n\u001b[1;32m     56\u001b[0m probs \u001b[38;5;241m=\u001b[39m epsilon_greedy_policy(Q, state, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, epsilon)\n\u001b[1;32m     57\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(probs)), p\u001b[38;5;241m=\u001b[39mprobs)\n\u001b[0;32m---> 59\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# as it's inherited from gym and not gymnasium, we receive only the done, instead of terminated and truncated. \u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Update TD\u001b[39;00m\n\u001b[1;32m     62\u001b[0m td_target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m discount \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(Q[next_state, :])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:60\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     57\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n\u001b[1;32m     61\u001b[0m     truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, terminated, truncated, info\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q = QLearning(episodes=15000, learning_rate=0.8, discount=0.95, epsilon=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "  L  |  L  |  L  |  L  |\n",
      "------------------------------------------------------------------------\n",
      "  L  |  L  |  L  |  L  |\n",
      "------------------------------------------------------------------------\n",
      "  L  |  L  |  L  |  L  |\n",
      "------------------------------------------------------------------------\n",
      "  L  |  L  |  L  |  L  |\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_policy_SARSA(Q, width, height):\n",
    "    switch_action = {\n",
    "        0: \"L\",\n",
    "        1: \"D\",\n",
    "        2: \"R\",\n",
    "        3: \"U\"\n",
    "    }\n",
    "    for j in range(height):\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        for i in range(width):\n",
    "            arr = np.array(Q[j*width+i])\n",
    "            act = np.argmax(arr)\n",
    "            a = switch_action[act]\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "\n",
    "print_policy_SARSA(q, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solution</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M2.883_PEC1_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
